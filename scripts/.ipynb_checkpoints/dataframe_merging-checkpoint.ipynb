{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b950cd4f",
   "metadata": {},
   "source": [
    "# Reading in Data, Merging Dataframes and Intro to GIS\n",
    "\n",
    "### In this lesson, we will:\n",
    " - Learn about relative file paths (../data/*.csv)\n",
    " - Practice loading in datasets saved in different file formats (.csv, excel, .shp)\n",
    " - Learn the various ways to merge dataframes (concat, merge(left, right, outer, inner))\n",
    " - Practice Merging Diabetes Prevalence in the State of Texas Datasets on both County Name and FIPS codes\n",
    " - Be introduced to the geopandas package for mapping and merge datasets with Shapefiles to Map the Data they contain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc18199",
   "metadata": {},
   "source": [
    "Let's begin by loading in all the relevant libraries we're going to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "from shapely.geometry import Point, Polygon\n",
    "from geopandas import datasets, GeoDataFrame, read_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab45f9c",
   "metadata": {},
   "source": [
    "## Reading in Data and Relative File Paths\n",
    "\n",
    "In order to read in datasets to turn them into dataframes (something that we did briefly last week) we need to know what format the data is saved in, what library or function to use to open that type of datafile, and where to direct the computer to find that data file. \n",
    "\n",
    "There are three different types of datafiles we are going to be working with today:\n",
    " - Comma Seperated Values files (.csv)\n",
    " - Excel files (.xlxs)\n",
    " - Shape files (.shp)\n",
    "\n",
    "We are going to start by reading in the *.csv file in our tutorial directory's /data folder.\n",
    "\n",
    "Currently, we are running our jupyter notebook file (.ipynb) from the scripts folder in the tutorial directory.\n",
    "\n",
    "The full directory for our tutorial would look something like this\n",
    "> \"User/Documents/dataframe_merging_tutorial/scripts/dataframe_merging.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4749085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can actually print our current working directory using the following code:\n",
    "os.getcwd()\n",
    "\n",
    "# Save the current working directory to an object\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory: {0}\".format(cwd))\n",
    "\n",
    "# Print the type of the returned object\n",
    "print(\"os.getcwd() returns an object of type: {0}\".format(type(cwd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49141ac6",
   "metadata": {},
   "source": [
    "It is important to know what your current working directory is before beginning to code so that you know how to arrange your folder file structure and direct your code through this structure to the relevant files and scripts.\n",
    "\n",
    "To direct your computer to relevant files in your directory's file structure, we are going to make use of relative file paths. While we could type out the full directory file path each time we want to call in a datafile from our directory, relative file paths allows us to start from our current notebook and work outwards. \n",
    "\n",
    "This is also helpful when you are sharing, or collaborating on, code with others so that each of your computers do not have to be set up completely the same way. Just the folder directory that you are coding within.\n",
    "\n",
    "Instead of needing to write this whole filepath to get to our jupyter notebook\n",
    "> \"User/Documents/dataframe_merging_tutorial/scripts/dataframe_merging.ipynb\"\n",
    "\n",
    "We can instead use the __\"../\"__ syntax to essentially start from __\"dataframe_merging.ipynb__ out of the __\"scripts\"\"__ folder, into the __\"dataframe_merging_tutorial\"__ folder and direct it to the __\"data\"__ folder.\n",
    "> \"../data\n",
    "\n",
    "The __\"../\"__ syntax tells the computer to move up one folder level from the python notebook you are working in. You can use it twice to go two folder levels out to the __\"Documents\"__ folder, then add a __\"/\"__ and the name of a folder in your documents folder to go into.\n",
    "\n",
    "You can use relative file paths to traverse your computer's entire file structure. We will be suing relative file paths to direct the computer towards the relevant data files we will be reading in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63e175b",
   "metadata": {},
   "source": [
    "### Let's try reading in a .csv file\n",
    "Use ../ to direct your computer to the data folder and the *.csv file inside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2edcc3",
   "metadata": {},
   "source": [
    "Function Documentation: [Pandas.read_csv()](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37927df1",
   "metadata": {},
   "source": [
    "Read in County GDP .csv data from Bureau of Economic Analysis\n",
    "\n",
    "Data Source: https://apps.bea.gov/iTable/iTable.cfm?reqid=70&step=1&acrdn=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the BEA gdp data by county csv \n",
    "gdp = pd.read_csv('../data/GDP_TX_COUNTY_2001_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05efffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get rid of reader and footer\n",
    "gdp = gdp[3:765]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b9108",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative to df splicing to get rid of footer\n",
    "gdp = pd.read_csv('../data/GDP_TX_COUNTY_2001_2019.csv', skipfooter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bf530",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34e870",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp = gdp[3:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ff8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eefc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter down the df to just include the county FIPS, unit of analysis (quantity index, gdp in thousands of dollars,\n",
    "#gdp in thousands of 2012 dollars)\n",
    "gdp_clean = gdp.filter(['GeoFIPS','Unit','2019'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380f7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Df currently is given in long-format, each entry a city with a different measure. City is given mutliple times, each\n",
    "#row only one type of measure. We want to pivot this long data to wide, giving us each city as a row and each measure\n",
    "#of interest as a column. Using .pivot(), we decide what column will be the row (index = county), which will be the \n",
    "#columns (the categorical values in unit),and which will be the value in these columns( all the values under 2019)\n",
    "\n",
    "gdp_wide = gdp_clean.pivot(index='GeoFIPS',columns='Unit',values='2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e0697",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ade18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the columns to make them easier to use\n",
    "gdp_wide = gdp_wide.rename(columns={'Quantity index':'quantity_index',\n",
    "                         'Thousands of chained 2012 dollars':'thousands_chained_2012_$_2019',\n",
    "                         'Thousands of dollars':'thousands_$_2019'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef9cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb1ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since county fips was saved as the index, its hard to access, so take the index and turn it into its own column\n",
    "gdp_wide['GeoFIPS']= gdp_wide.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d1c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdp_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed89c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For whatever reason, the county fips \"str\" also includes the \"\" around it, we need to strip these off using list\n",
    "#comprehension\n",
    "\n",
    "fips_txt_list = []\n",
    "\n",
    "for x in gdp_wide['GeoFIPS']:\n",
    "    fips_txt_list.append(x[1:-1])\n",
    "\n",
    "#Take the clipped county fips \"str\" list and add it as a new column to the df\n",
    "gdp_wide['fips_clipped'] = fips_txt_list\n",
    "\n",
    "gdp_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this wide gdp df as a .csv for future use\n",
    "gdp_wide.to_csv('../data/gdp_wide_2019_example.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2319b8",
   "metadata": {},
   "source": [
    "### Let's try opening an excel file\n",
    "\n",
    "read_excel has different arguments that become important when you're dealing with excel files that have more than just column info in them, as well as multiple sheets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9bcb1",
   "metadata": {},
   "source": [
    "Practice reading in Excel file of RUCA Communting Codes from USDA ERS - Given at Census Tract Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d0378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the RUCA codes from the USDA ERS excel file\n",
    "ruca_commuting = pd.read_excel('../data/ruca2010revised.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c837b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the RUCA codes from the USDA ERS excel file\n",
    "ruca_commuting = pd.read_excel('../ruca2010revised.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ada9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruca_commuting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549885c6",
   "metadata": {},
   "source": [
    "The headers from our excel file are all messed up, not correspinding to our acutal column and varibale names. We see that info is actually in row 0. We want to use the header argument in read_csv to tell the computer to make that row our varaible name row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df98983",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the RUCA codes from the USDA ERS excel file\n",
    "ruca_commuting = pd.read_excel('../ruca2010revised.xlsx',header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88780dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruca_commuting.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a570c14",
   "metadata": {},
   "source": [
    "Let's look at the datatypes of the variables we pulled in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruca_commuting.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b02444",
   "metadata": {},
   "source": [
    "Say that pandas read in the data as a different datatype than we need to work with. We could tell pandas to covert different columns to different datatypes when we read in the excel file using the converters argument and adding a dictionary with the column name as the key and the datatype we want it converted to as the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a3e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the RUCA codes from the USDA ERS excel file\n",
    "ruca_commuting = pd.read_excel('../ruca2010revised.xlsx',header=1,\n",
    "                     converters={'State-County FIPS Code':str,\n",
    "                                 \"State-County-Tract FIPS Code (lookup by address at http://www.ffiec.gov/Geocode/)\":str,\n",
    "                                 'Primary RUCA Code 2010':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa470be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruca_commuting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruca_commuting.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26405fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename unwieldy columns to make them more navigable and manipulatable in pandas\n",
    "ruca_commuting = ruca_commuting.rename(columns={\"State-County-Tract FIPS Code (lookup by address at http://www.ffiec.gov/Geocode/)\"\n",
    "                              :'tract_fips','State-County FIPS Code':'county_fips','Primary RUCA Code 2010'\n",
    "                              :'primary_ruca_2010',\"Secondary RUCA Code, 2010 (see errata)\":'secondary_ruca_2010'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fccc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruca_commuting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74929693",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruca_commuting_clean  = ruca_commuting.filter([\"county_fips\",\"Select State\",\"Select County\",\"tract_fips\",\n",
    "                                              \"primary_ruca_2010\",\"secondary_ruca_2010\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcaa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruca_commuting_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this cleaned RUCA df as a .csv for future use\n",
    "ruca_commuting_clean.to_csv('../data/ruca_commuting_example.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c7d04",
   "metadata": {},
   "source": [
    "# Reading in and Merging Multiple Datasets\n",
    "\n",
    "## Diabetes Prevalence\n",
    "Data for 2019, earlier annual datasets available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b051d84",
   "metadata": {},
   "source": [
    "Datasets Downloaded from the Center for Disease Control's Diabete's Atlas: https://gis.cdc.gov/grasp/diabetes/DiabetesAtlas.html# \n",
    "\n",
    "This link leads to an interactive US Diabetes Surveillance System Mapper that allows you to query data from all of the US based on State and County and for Diabetes Prevalence as a proportion of the total geography population, as a population count, or as a geography rank for the chosen scale of geography.\n",
    "\n",
    "I took the proportion, count and rank variables for all counties in the State of Texas and downloaded separate csv files for each. I will merge all of them, merge them with our county shapefiles and create a layer of data for both the state of Texas and the 5-County Central Texas Region to be paired with our other relevant map layers.\n",
    "\n",
    "Note that the State of Texas's Diabetes prevalence at the county level is already a static map we have located, and that the Surveillance System Mapper platfrom allows you to reproduce this map; however, what we are doing is taking this data and making it a layer to be able to load with other layers for analysis, something these already existing maps do not allow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303baf14",
   "metadata": {},
   "source": [
    "### Read in Diabetes Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a11706",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_percent = pd.read_csv('../data/DiabetesAtlasCountyDataPercentage.csv')\n",
    "diabetes_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085970bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_percent = pd.read_csv('../data/DiabetesAtlasCountyDataPercentage.csv',header=2)\n",
    "diabetes_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_percent = pd.read_csv('../data/DiabetesAtlasCountyDataPercentage.csv', header=2, skipfooter=1)\n",
    "diabetes_count = pd.read_csv('../data/DiabetesAtlasCountyDataCount.csv', header=2, skipfooter=1)\n",
    "diabetes_rank = pd.read_csv('../data/DiabetesAtlasCountyDataRank.csv', header=2,skipfooter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b989fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd09f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3095d79a",
   "metadata": {},
   "source": [
    "### Simple Merging: Merge on \"County Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87bcef8",
   "metadata": {},
   "source": [
    "We can merge using the pd.concat(), which is a way of adding dataframes together. It does require that the row order of the dataframes is identical and that all the dataframes hae the same number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb58ffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.concat([diabetes_percent,diabetes_count,diabetes_rank],axis=1, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fcd502",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad31d91d",
   "metadata": {},
   "source": [
    "A lot of dataframes we are working with may have missing data, or rows in different orders, or non-completely overlapping rows with repeats. In this case, we want to use the pd.merge function which asks you to choose which dataframes you are merging and the column shared between the two that you want to merge on. \n",
    "\n",
    "You also have to choose what type of merge you are going to use: __left, right, inner, or outer__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6635ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_percent_count_county = diabetes_percent.merge(diabetes_count, left_on='County', right_on='County',\n",
    "                                                       how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_percent_count_county"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f171e4",
   "metadata": {},
   "source": [
    "### Complex Merging: Transform FIPS datatype and Merge on FIPS code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f362f06",
   "metadata": {},
   "source": [
    "The above merge was easy because all the rows mathced and county name was the same since all the dataframes came from the same source.\n",
    "\n",
    "Often we'll have columns in separate dataframes that need to be merged on but that are in different file formats that take more data cleaning and processing to get to the point of merging.\n",
    "\n",
    "For our geographic data, this is going to be the case and specifically with FIPS codes. FIPS codes will be read in in a variety of formats, often needing to be strings to begin with so that we can clean and splice them to sort by sub-fips codes. \n",
    "\n",
    "Then they need to be converted to int64 datatypes to be merged. This is because the dataframe doesn't allow us to transform columns if there is any missing data in them coded as \"NaN\". So we ned to convert all data values in the FIPS column to int64 in order to handle missing values and merge on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function that will convert the 'str' fips code to 'int64' for merging, we use int64 because it can handle\n",
    "#NaN data whereas 'int' cannot\n",
    "\n",
    "def fips_2_int(df,fips_col,new_col):\n",
    "        df[new_col] = [int(x) if type(x)==str else \n",
    "                       (np.nan if math.isnan(x)==True else(int(x) if type(x)==float else int(x))) \n",
    "                       for x in df[fips_col]]\n",
    "        df[new_col] = df[new_col].astype('Int64')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d19b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_2_int(diabetes_percent,'CountyFIPS','FIPS_int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17af38",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_2_int(diabetes_count,'CountyFIPS','FIPS_int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb11d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_2_int(diabetes_rank,'CountyFIPS','FIPS_int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e76fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_1 = diabetes_percent.merge(diabetes_count, left_on='FIPS_int64', right_on='FIPS_int64', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba966c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da81bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_2 = diabetes_1.merge(diabetes_rank, left_on='FIPS_int64', right_on='FIPS_int64', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a79d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec3c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_clean = diabetes_2.filter(['County','FIPS_int64','Number','Percentage','Rank'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339501a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0a5250",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_clean.to_csv(\"../data/diabetes_clean.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3135b85a",
   "metadata": {},
   "source": [
    "## Reading and Merging in Geographic Data to Map Diabetes Prevalence\n",
    "\n",
    "The next step is to make this diabetes data mappable at the county level for the state of Texas. To do this, we need to read in the Tiger/Line shape file that has the geographic shapes of each county in the US. \n",
    "\n",
    "We'll merge our diabetes dataframe with this shape file on the county_fips code. We'll do a left merge to only keep those county geometries for the Texas counties listed in the diabetes dataset, getting rid of all the other county geometries for the whole country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_shp = \"../data/tl_2021_us_county/tl_2021_us_county.shp\"\n",
    "data_2 = gpd.read_file(county_shp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b97e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fips_2_int(data_2,'GEOID','FIPS_int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2.plot(facecolor='none',edgecolor='black',linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdf766",
   "metadata": {},
   "source": [
    "Let's look at what the geometry data we are going to merge in looks like by making a new dataframe that is only the 5-county Austin MSA region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0882c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_county_shp = data_2[(data_2[\"FIPS_int64\"]==48453)|(data_2[\"FIPS_int64\"]==48021)|\n",
    "                        (data_2[\"FIPS_int64\"]==48055)|(data_2[\"FIPS_int64\"]==48209)|\n",
    "                        (data_2[\"FIPS_int64\"]==48491)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754efff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_county_shp.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_county_shp = five_county_shp.to_crs(epsg=3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec56d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_county_shp.plot(facecolor='none',edgecolor='black',linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3217ce",
   "metadata": {},
   "source": [
    "## Merging Dataframes with Shapefiles for Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f3bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_county = diabetes_clean.merge(data_2, left_on='FIPS_int64', right_on='FIPS_int64', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5783c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_county_shp = diabetes_county.filter(['County','FIPS_int64','Number','Percentage','Rank','geometry'],\n",
    "                                             axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcbe17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_county_shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_geo = gpd.GeoDataFrame(diabetes_county_shp, crs=\"EPSG:3857\", geometry='geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8855226",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_geo.to_file('../data/diabetes_texas_counties.shp',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = diabetes_geo.plot(column='Percentage',\n",
    "                        zorder=2,\n",
    "                   figsize=(15, 10),\n",
    "                   cmap='Blues',\n",
    "                   scheme='quantiles',\n",
    "                   edgecolor='black',\n",
    "                        alpha=0.7,\n",
    "                   legend=True)\n",
    "\n",
    "\n",
    "ax.axis('off')\n",
    "plt.suptitle('Diabetes Prevalence in the State of Texas',y=.94, fontsize=25)\n",
    "plt.title('Proportion of Each County with Diabetes')\n",
    "\n",
    "#plt.savefig('../images/texas_wide_diabetes_percentage_county_prevalence.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
